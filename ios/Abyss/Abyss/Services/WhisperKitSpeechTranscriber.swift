import Foundation
import AVFoundation
#if canImport(WhisperKit)
import WhisperKit
#endif

/// WhisperKit-based on-device speech transcriber.
/// Streams partial transcripts while recording, returns final on stop.
final class WhisperKitSpeechTranscriber: SpeechTranscriber, @unchecked Sendable {
    private let lock = NSLock()

    private var _isListening = false
    var isListening: Bool {
        lock.withLock { _isListening }
    }

    private var partialContinuation: AsyncStream<String>.Continuation?
    private var _partials: AsyncStream<String>?
    private var audioEngine: AVAudioEngine?
    private var accumulatedText: String = ""

    #if canImport(WhisperKit)
    private var whisperKit: WhisperKit?
    private var whisperKitInitTask: Task<WhisperKit?, Never>?
    private var audioBuffers: [Float] = []
    private var inputSampleRate: Double = 16000
    private let targetSampleRate: Double = 16000
    private var partialTranscriptionTask: Task<Void, Never>?
    private var partialTranscriptionInFlight = false
    private var lastPartialSampleCount = 0
    private let partialDebounceNanoseconds: UInt64 = 250_000_000
    #endif

    var partials: AsyncStream<String> {
        lock.withLock {
            if let existing = _partials { return existing }
            let (stream, continuation) = AsyncStream<String>.makeStream()
            self._partials = stream
            self.partialContinuation = continuation
            return stream
        }
    }

    init() {}

    func preload() async {
        #if canImport(WhisperKit)
        await ensureWhisperKitLoaded()
        #endif
    }

    func start() async throws {
        // Request microphone permission
        let session = AVAudioSession.sharedInstance()
        try session.setCategory(.record, mode: .measurement, options: .duckOthers)
        try session.setActive(true)

        #if canImport(WhisperKit)
        // Ensure the model is ready (or attempted) before recording starts.
        await ensureWhisperKitLoaded()
        audioBuffers = []
        partialTranscriptionTask?.cancel()
        partialTranscriptionTask = nil
        partialTranscriptionInFlight = false
        lastPartialSampleCount = 0
        #endif

        // Recreate the partial stream
        let (stream, continuation) = AsyncStream<String>.makeStream()
        lock.withLock {
            self._partials = stream
            self.partialContinuation = continuation
            self._isListening = true
            self.accumulatedText = ""
        }

        // Start the audio engine for capture
        let engine = AVAudioEngine()
        let inputNode = engine.inputNode

        // Use inputFormat (not outputFormat) to get the actual hardware capture format.
        // If the sample rate is invalid (e.g. 0 Hz before session activation), fall back to 48kHz mono.
        let hwFormat = inputNode.inputFormat(forBus: 0)
        let format: AVAudioFormat
        if hwFormat.sampleRate > 0 {
            format = hwFormat
        } else {
            format = AVAudioFormat(
                commonFormat: .pcmFormatFloat32,
                sampleRate: 48000,
                channels: 1,
                interleaved: false
            ) ?? hwFormat
            print("‚ö†Ô∏è Hardware format had 0 Hz sample rate ‚Äî using 48kHz fallback")
        }

        #if canImport(WhisperKit)
        inputSampleRate = format.sampleRate
        print("üé§ Audio engine started. Sample rate: \(inputSampleRate) Hz")
        // Emit immediate feedback so user sees the mic is working
        partialContinuation?.yield("Listening‚Ä¶")
        #else
        // Without WhisperKit, still show feedback
        partialContinuation?.yield("Listening‚Ä¶ (no transcription)")
        #endif

        inputNode.installTap(onBus: 0, bufferSize: 4096, format: format) { [weak self] buffer, _ in
            guard let self else { return }
            let channelData = buffer.floatChannelData?[0]
            let frameLength = Int(buffer.frameLength)

            if let channelData {
                let samples = Array(UnsafeBufferPointer(start: channelData, count: frameLength))
                #if canImport(WhisperKit)
                self.lock.withLock {
                    self.audioBuffers.append(contentsOf: samples)
                }
                self.scheduleLivePartialTranscription()
                #else
                // Without WhisperKit, emit buffer level as placeholder
                let rms = samples.reduce(0) { $0 + $1 * $1 } / Float(frameLength)
                if rms > 0.001 {
                    self.partialContinuation?.yield("[audio level: \(String(format: "%.4f", rms))]")
                }
                #endif
            }
        }

        engine.prepare()
        try engine.start()
        self.audioEngine = engine
    }

    #if canImport(WhisperKit)
    private func ensureWhisperKitLoaded() async {
        if lock.withLock({ whisperKit != nil }) {
            return
        }

        if let existingTask = lock.withLock({ whisperKitInitTask }) {
            _ = await existingTask.value
            return
        }

        let loadTask = Task<WhisperKit?, Never> {
            print("üì± Initializing WhisperKit with base.en model...")
            do {
                let kit = try await WhisperKit(model: "base.en")
                print("‚úÖ WhisperKit loaded successfully")
                return kit
            } catch {
                print("‚ùå WhisperKit initialization failed: \(error)")
                print("‚ö†Ô∏è Continuing without transcription - audio will be captured but not transcribed")
                return nil
            }
        }

        lock.withLock {
            whisperKitInitTask = loadTask
        }

        let loadedKit = await loadTask.value
        lock.withLock {
            if let loadedKit {
                whisperKit = loadedKit
            }
            whisperKitInitTask = nil
        }
    }

    /// Resample to 16kHz for WhisperKit (iPhone often captures at 48kHz).
    private func resampleTo16k(_ samples: [Float], sourceRate: Double) -> [Float] {
        guard sourceRate != targetSampleRate, sourceRate > 0 else { return samples }
        let ratio = sourceRate / targetSampleRate
        let targetCount = Int(Double(samples.count) / ratio)
        guard targetCount > 0 else { return [] }
        var out = [Float](repeating: 0, count: targetCount)
        for i in 0..<targetCount {
            let srcIndex = Double(i) * ratio
            let idx = Int(srcIndex)
            if idx + 1 < samples.count {
                let t = Float(srcIndex - Double(idx))
                out[i] = samples[idx] * (1 - t) + samples[idx + 1] * t
            } else {
                out[i] = samples[min(idx, samples.count - 1)]
            }
        }
        return out
    }

    private func transcribePartial() async {
        guard let wk = whisperKit else {
            print("üîç [PARTIAL] transcribePartial() ‚Äî no WhisperKit, skipping")
            return
        }
        let snapshot: (samples: [Float], sampleRate: Double, sampleCount: Int)? = lock.withLock {
            guard _isListening, !partialTranscriptionInFlight else {
                // Don't spam; this fires often and the guard is expected to fail sometimes
                return nil
            }
            let sampleCount = audioBuffers.count
            let minNewSamples = max(Int(inputSampleRate * 0.2), 1600)
            guard sampleCount - lastPartialSampleCount >= minNewSamples else { return nil }

            partialTranscriptionInFlight = true
            return (audioBuffers, inputSampleRate, sampleCount)
        }
        guard let snapshot else { return }
        print("üîç [PARTIAL] transcribePartial() ENTER ‚Äî bufferCount=\(snapshot.sampleCount) at \(snapshot.sampleRate)Hz")
        defer {
            lock.withLock {
                partialTranscriptionInFlight = false
                lastPartialSampleCount = snapshot.sampleCount
            }
            print("üîç [PARTIAL] transcribePartial() DONE ‚Äî inFlight cleared")
        }

        let samples = snapshot.samples
        // Allow early partials so transcript feels live while user is speaking.
        let minSamplesAtInputRate = Int(snapshot.sampleRate * 0.35)
        guard samples.count > minSamplesAtInputRate else {
            print("üîç [PARTIAL] skipping ‚Äî too few samples (\(samples.count) < \(minSamplesAtInputRate))")
            return
        }

        let forWhisper = resampleTo16k(samples, sourceRate: snapshot.sampleRate)
        guard forWhisper.count > 3200 else {
            print("üîç [PARTIAL] skipping ‚Äî resampled too small (\(forWhisper.count))")
            return
        }

        print("üîç [PARTIAL] calling wk.transcribe ‚Äî \(forWhisper.count) samples @16kHz")
        do {
            let results = try await wk.transcribe(audioArray: forWhisper)
            let rawText = results.first?.text ?? "nil"
            print("üîç [PARTIAL] wk.transcribe returned ‚Äî text='\(rawText)'")
            if let text = results.first?.text, !text.isEmpty {
                let trimmed = text.trimmingCharacters(in: .whitespacesAndNewlines)
                // Skip empty or meaningless outputs
                if trimmed.count > 1 && trimmed != "Listening‚Ä¶" {
                    lock.withLock { accumulatedText = trimmed }
                    partialContinuation?.yield(trimmed)
                }
            }
        } catch {
            print("üîç [PARTIAL] wk.transcribe THREW: \(error)")
        }
    }

    private func scheduleLivePartialTranscription() {
        let shouldSchedule = lock.withLock {
            guard partialTranscriptionTask == nil else { return false }
            return true
        }
        guard shouldSchedule else { return }

        let task = Task { [weak self] in
            guard let self else { return }
            try? await Task.sleep(nanoseconds: self.partialDebounceNanoseconds)
            guard !Task.isCancelled else { return }
            await self.transcribePartial()
            self.lock.withLock {
                self.partialTranscriptionTask = nil
            }
        }

        lock.withLock {
            partialTranscriptionTask = task
        }
    }
    #endif

    func stop() async throws -> String {
        let engineRunning = audioEngine != nil
        let listeningNow = lock.withLock { _isListening }
        print("üõë [STOP-1] stop() ENTER ‚Äî isListening=\(listeningNow), audioEngine=\(engineRunning)")

        audioEngine?.inputNode.removeTap(onBus: 0)
        audioEngine?.stop()
        audioEngine = nil
        print("üõë [STOP-2] audio engine torn down (removeTap + stop)")

        lock.withLock {
            _isListening = false
        }

        #if canImport(WhisperKit)
        let hadTask = lock.withLock { partialTranscriptionTask != nil }
        lock.withLock {
            partialTranscriptionTask?.cancel()
            partialTranscriptionTask = nil
        }
        print("üõë [STOP-2b] partial task cancelled (had task=\(hadTask))")

        // Wait for any in-flight partial transcription to finish before running the final pass.
        // WhisperKit is not safe to call concurrently ‚Äî a concurrent call causes a hang/deadlock.
        let inFlightAtStart = lock.withLock { partialTranscriptionInFlight }
        print("üõë [STOP-3] partial wait ‚Äî partialTranscriptionInFlight=\(inFlightAtStart) at entry")
        var waitMs = 0
        while lock.withLock({ partialTranscriptionInFlight }) && waitMs < 6000 {
            try? await Task.sleep(nanoseconds: 100_000_000) // 100ms
            waitMs += 100
            if waitMs % 1000 == 0 {
                print("üõë [STOP-3-POLL] still waiting for partial‚Ä¶ \(waitMs)ms elapsed")
            }
        }
        let timedOutWaiting = waitMs >= 6000
        print("üõë [STOP-4] partial wait done ‚Äî waited \(waitMs)ms, timedOut=\(timedOutWaiting)")
        if timedOutWaiting {
            print("‚ö†Ô∏è [STOP-4-WARN] timed out waiting for in-flight partial ‚Äî proceeding with accumulated text")
        }

        // Final transcription pass on all accumulated audio (resampled to 16kHz)
        let samples: [Float] = lock.withLock { audioBuffers }
        let whisperLoaded = lock.withLock { whisperKit != nil }
        print("üõë [STOP-4b] snapshot ‚Äî samples=\(samples.count) at \(inputSampleRate)Hz, whisperKitLoaded=\(whisperLoaded)")

        // If we have no samples, the audio engine never captured anything
        if samples.isEmpty {
            print("‚ö†Ô∏è [STOP-4c] NO audio captured ‚Äî returning early")
            let finalText = lock.withLock { accumulatedText }
            partialContinuation?.finish()
            try? AVAudioSession.sharedInstance().setActive(false, options: .notifyOthersOnDeactivation)
            print("üõë [STOP-7] stop() returning (no audio) ‚Äî '\(finalText)'")
            return finalText.isEmpty ? "[No audio captured]" : finalText
        }

        if let wk = whisperKit, samples.count > 1600 {
            let forWhisper = resampleTo16k(samples, sourceRate: inputSampleRate)
            print("üõë [STOP-5] starting final WhisperKit transcription ‚Äî \(forWhisper.count) samples @16kHz (8s timeout)")
            if forWhisper.count > 1600 {
                // Race WhisperKit against an 8-second timeout so we never hang in .transcribing.
                let results: [TranscriptionResult]? = await withTaskGroup(
                    of: [TranscriptionResult]?.self
                ) { group in
                    group.addTask { try? await wk.transcribe(audioArray: forWhisper) }
                    group.addTask {
                        try? await Task.sleep(nanoseconds: 8_000_000_000)
                        return nil
                    }
                    let first = await group.next() ?? nil
                    group.cancelAll()
                    return first
                }

                if let results {
                    if let text = results.first?.text, !text.isEmpty {
                        let trimmed = text.trimmingCharacters(in: .whitespacesAndNewlines)
                        print("üõë [STOP-6] final transcription DONE ‚Äî '\(trimmed)'")
                        lock.withLock { accumulatedText = trimmed }
                    } else {
                        print("üõë [STOP-6] final transcription returned EMPTY result")
                    }
                } else {
                    print("üõë [STOP-6] final transcription TIMED OUT (8s) ‚Äî using accumulated partial")
                }
            } else {
                print("üõë [STOP-5-SKIP] resampled count \(forWhisper.count) too small ‚Äî skipping inference")
            }
        } else if whisperKit == nil {
            print("üõë [STOP-5-SKIP] WhisperKit not loaded ‚Äî skipping final transcription")
        } else {
            print("üõë [STOP-5-SKIP] samples.count \(samples.count) <= 1600 ‚Äî skipping final transcription")
        }
        #endif

        let finalText = lock.withLock { accumulatedText }
        partialContinuation?.finish()

        // Reset audio session
        try? AVAudioSession.sharedInstance().setActive(false, options: .notifyOthersOnDeactivation)

        print("üõë [STOP-7] stop() returning '\(finalText)'")
        return finalText
    }
}
